{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An Introduction to Neural Networks\n",
    "\n",
    "Neural networks are one of the hottest areas of machine learning. Neural network models attract the most press because of their relationship to the human brain enabling the anthropomorphism of artificial intelligence further. Neural networks aim to emulate the neurons of the brain with perceptrons linking them in networks.\n",
    "\n",
    "Humans still maintain the edge in the comparison of human and computer intelligence and a look at the wetware specs of the brain and the hardware specs of computers gives us an idea as to why.\n",
    "\n",
    "### Human Brain\n",
    "* Neuron Switching Time:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$0.001\\text{s}$ ($1\\text{ms}$)\n",
    "* Number of Neurons:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$10^{10}$\n",
    "* Connections per Neuron:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$10^4-10^5$\n",
    "* Scene Recognition Time:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$~0.1\\ Second$\n",
    "\n",
    "### Computer\n",
    "* Transistor Switching Time:&nbsp;&nbsp;&nbsp;&nbsp;$~333\\ picoseonds$ (3GHz Processor)\n",
    "* Transistors per Chip:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$8 - 20\\ billion$\n",
    "* Connections per Transisor:&nbsp;&nbsp;&nbsp;$~10$\n",
    "* Scene Recognition Time:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;No general scene recognition progamme yet exists (The _Inception_ model is getting pretty close).\n",
    "\n",
    "We may see that computer hardware wins the comparison hands down in switching time and is starting to win in terms of number of transistors/neurons but the human brain seems way more powerful as a general purpose computing unit at the moment. The power of this comes through the connections per neuron. It is currently not forseen that computing hardware of the current paradigm will ever overtake the brain in this respect. The connections between neurons (albeit slow, exhchanging information via synapses) are where the brain learns and maintains its computational superiority.\n",
    "\n",
    "Furthermore the brain manages this power with power consumption on the order of 100 Watts whereas the computers used to train and build the leading neural network models consume far far more energy. The brain manages to achieve massive parallelism through the connections of neurons from one to another and thereby achieves remarkable efficiency and understanding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Neuron and the Perceptron\n",
    "The software equivalent of the neuron is the perceptron. A single neuron is able to encode a single binary threshold which could be sufficent for a simple binary classification model. The neuron either fires or doesn't. In the case that the incoming stimulus is sufficient to cause the neuron to fire an action potential is sent down the axon and down the dendrites which then passes the stimulus on to the neurons connected to firing neuron. This threshold will differ from neuron to neuron as will the number and paths of the connections between neurons.\n",
    "\n",
    "![Neuron Diagram](./img/neuron.png \"A Neuron\")\n",
    "<center><span style=\"color:gray\">*Source: Wikimedia Commons*</span></center>\n",
    "\n",
    "The perceptron tries to emulate this mathematically by taking in a series of inputs summing them (scaled by some weights) and comparing them to a threshold.\n",
    "\n",
    "![Perceptron Diagram](./img/perceptron.png \"The Perceptron\")\n",
    "<center><span style=\"color:gray\">*Source: Wikimedia Commons (Edited)*</span></center>\n",
    "\n",
    "In the diagram the threshold is given by the constant $\\theta$ which is also referred to as the *bias*. The inputs are given by the *n*-vector ***x***. The *x* values are then multiplied by their respective weights and summed before the result is passed through the step function which then leads to a binary output ($1$ '*fire*' or $0$ '*do not fire*'). This step function is known as the *activation function* since it determines the rule as to whether or not the perception '*fires*'.\n",
    "\n",
    "Mathematically the perceptron (in the case of the purest neuron emulation) undertakes the following calculation:\n",
    "\n",
    "$$output = \\begin{cases} 1 &\\mbox{if    } \\left(w_0 + \\sum_{i=1}^n w_ix_i\\right)>0 \\\\ \n",
    "0 & \\mbox{otherwise} \\end{cases}$$\n",
    "\n",
    "The output of the perceptron can then be passed forwards to other perceptrons akin to the action potential passing from the dendrites of one neuron to the next neuron through the chemical mediation of a synapse (which is very slow by computing standards). In this way perceptrons are built up to generate a neural network.\n",
    "\n",
    "Taking the single perceptron as an example we may see that it characterises a hyperplane which implements a binary classification. The gradient of the hyperplane is determined by the weights vector $\\boldsymbol{w}$ and its position is given by the bias $w_0 = \\theta$. The two-dimensional case leads to classification according to a single straight line which clearly shows the limitations of the perceptron as a classifier - it can only separate two regions and only then in a linear way. The most often cited of what this cannot deal with is the case of parity (a.k.a. the *exclusive-or* or `XOR` function). However, we will see later that neural networks can get around this issue to be one of the most flexible learners out there.\n",
    "\n",
    "The 'learning' of anything in neural networks is mediated through the weights. The weights encode all knowledge. The learning process begins with the weights initialised at small random numbers and then as examples are seen and errors noted the weights are updated in order to reduce the error. This can be thought of as the strengthening of pathways in the brain. The more a path is used to preidct the output the greater the mangnitude of the weight of that link in the network. This comparison is more clear cut in the case of a neural network where the inputs to one perceptron are the outputs of other perceptrons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coding A Perceptron\n",
    "We will use python to set up a perceptron and gain an idea of how to train it. To do this we will use NumPy the go to package for scientific computing in python.\n",
    "\n",
    "There are many packages for building neural networks in Python the most popular being SciKit-Learn, Keras, Theano and TensorFlow however we will initially build our models from scratch for pedagogic suitability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In keeping with the idea that a perceptron simply learns a hyperplane we will set up a dataset that based on a hyperplane and we will try to learn it with a single perceptron."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a Dataset\n",
    "To set up the dataset we will simply take points in three-dimensional space and classify them according to the hyperplane defined below. Below the hyperplane the points will be coded as 0 and above they will be encoded as 1.\n",
    "\n",
    "$$-6.2x+0.7y+4.3z=0.5$$\n",
    "\n",
    "The dataset will be created by generating random sets of numbers and then determining the output according to the hyperplane above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 1000 * np.random.uniform(size=(10000))\n",
    "y = 1000 * np.random.uniform(size=(10000))\n",
    "z = 1000 * np.random.uniform(size=(10000))\n",
    "output = -6.2*x + 0.7*y + 4.3*z > 500\n",
    "output = output.astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All being well the dataset should yield approximately 32% positive and 68% negative outcomes. Let us place all of the data in a Pandas DataFrame so that we may see and manage the data in tabular form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>418.971688</td>\n",
       "      <td>273.278110</td>\n",
       "      <td>838.785064</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>156.197645</td>\n",
       "      <td>262.411708</td>\n",
       "      <td>941.085832</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>560.389763</td>\n",
       "      <td>862.051968</td>\n",
       "      <td>727.246509</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>300.741245</td>\n",
       "      <td>985.352250</td>\n",
       "      <td>417.604131</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>80.147322</td>\n",
       "      <td>418.903629</td>\n",
       "      <td>395.570943</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            x           y           z  Target\n",
       "0  418.971688  273.278110  838.785064     1.0\n",
       "1  156.197645  262.411708  941.085832     1.0\n",
       "2  560.389763  862.051968  727.246509     0.0\n",
       "3  300.741245  985.352250  417.604131     1.0\n",
       "4   80.147322  418.903629  395.570943     1.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.DataFrame(data={'x':x,'y':y,'z':z,'Target':output})\n",
    "data = data[['x','y','z','Target']]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our dataset and an idea of the target function we may set up our perceptron. We initialise the weights vector and for simplicity we then build a function which will generate a prediction given the input vector and weights vector which includes the bias (hence why it is of size `NUM_INPUTS + 1`). The weights vector is initialised to small random values as we do not wish to impose any siginificant initial model or relationship before the model has been shown data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_INPUTS = 3\n",
    "weights = np.random.normal(0, 0.2, size=NUM_INPUTS + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When generating a prediction we must generate a binary output. This is done by comparing the sum of the product of the weights and the inputs to the threshold. This threshold is learned and is given by $w_0$. This is why $-1$ is prepended to the input vector so that the threshold is taken from the sum of the other weights and inputs so that generic comparison to may be undertaken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perceptron_prediction(input_vector, weights):\n",
    "    inputs = [-1, *input_vector]\n",
    "    prediction = float(np.dot(inputs, weights) > 0)\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the perceptron manages to learn the model exactly then we will expect the following weights vector.\n",
    "\n",
    "$$\\boldsymbol{w}=\\begin{bmatrix}w_0 \\\\ w_1 \\\\ w_2 \\\\ w_3\\end{bmatrix}=\\begin{bmatrix}500 \\\\ -6.2 \\\\ 0.7 \\\\ 4.3\\end{bmatrix}$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
