{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Multi-Layer Perceptron\n",
    "Any neural network is simply a series of perceptrons feeding into one another. The perceptrons are arranged in layers with the outputs of a previous layer being the inputs into the next layer.\n",
    "\n",
    "With this in mind it seems that we did much of the heavy lifting in our [first notebook](./01. The Perceptron.ipynb) concerning the percepton - the single neuron emulating unit that is the building block of a neural network.\n",
    "\n",
    "In this workbook we will consider the simplest case of a fully-connected network. That is a network where each node (percepton) in a given layer is connected to every node in the subsequent layer. This means that the output of a perceptron in the first layer will therefore be an input into each perceptron in the next layer.\n",
    "\n",
    "![Neural Network Diagram](./img/neural-network.png \"A Fully Connected Neural Network\")\n",
    "\n",
    "## Expressiveness of Neural Networks\n",
    "\n",
    "## Training a Neural Network vs Training a Perceptron\n",
    "Similarly to the case of the vanilla perceptron we considered before, the values passed as inputs to each layer (and therefore each perceptron in that layer) will have weights associated with them. These weights will be the variables which we will vary as part of the training process with the ultimate goal of loss minimisation.\n",
    "\n",
    "In contrast to a single perceptron, a multi-layer perceptron must be trained in a more complex way since the weights in the first layer only have an indirect impact on the output albeit through many channels in a fully connected network. This indirect influence is difficult to trace through should the step activation function be kept. The discontinuous function is not differentiable and hence we are unable to perform what's known as backpropagation to calculate the impact of early layer weights on the ultimate output. Backpropagation is the algorithmic way of training a neural network. Errors in the output are traced back through the layers of the network and the weights are updated in turn in order to reduce the error. We will consider this process in detail later. \n",
    "\n",
    "In order to undertake backpropagation the error, or more specifically the model prediction, needs to be differentiable with respect to the weights. In order to achieve this, a smooth (differentiable) activation function is used in place of the perceptron step function.\n",
    "\n",
    "These functions are known as activation functions because they play the role of the activation threshold in the perceptron or the neuron being emulated. Popular choices are the sigmoid function, tanh and the Rectified Linear Unit function (ReLU).\n",
    "\n",
    "We will discuss the mathematics of backpropagation in detail [later](#backpropagation).\n",
    "\n",
    "\n",
    "## Activation Functions\n",
    "Activation functions determine how a perceptron produces its output. Unlike neurons, perceptrons are not limited to binary output and hence may produce any real number as their output. The choice of activation function can therefore shape this output by shaping the relationship between inputs and the output beyond the linear way in which weights enter the equation. This relationship change can be made for computational ease, to match the relationship we are trying to estimate or to aid in training the model.\n",
    "\n",
    "In the case of the neurons in our brains that the perceptrons aim to emulate, activation functions play the role of the activation threshold. In the brain the threshold is placed on a concentration gradient across a membrane which much reach a certain strength before an activation potential will be passed along the neuron. In our case, working in the mathematics of real numbers we need not limit ourselves to binary output. We may consider any output on the real domain. This can be considered the number of action potentials per second should one wish to maintain the biological parallels.\n",
    "\n",
    "### The Sigmoid Function\n",
    "A binary classifier may aim to place a probability on the given input belonging to a given class. This will mean that the outputs must, by way of being probabilities, fall in the unit interval ($o \\in [0,1]$). The sigmoid function will always produce outputs that are between 0 and 1 and can therefore be considered valid probabilities. The functional form of the sigmoid function is given below.\n",
    "\n",
    "$$\\sigma(x)=\\frac{1}{1+e^{-x}}$$\n",
    "\n",
    "<small><center><span style=\"color:gray\">*The Sigmoid Function*</span></center></small>\n",
    "\n",
    "\n",
    "Note that this function is strictly increasing in $x$ and varies from $0$ when $x$ is $-\\infty$ and 1 when $x$ is $\\infty$. It is smooth, defined for all real numbers and differentiable.\n",
    "\n",
    "Infact the differential takes a rather nice form which is one reason for the function's popularity. This will help us when training the model later on.\n",
    "\n",
    "\n",
    "$$\\sigma(x)=\\frac{1}{1+e^{-x}}$$\n",
    "<small><center><span style=\"color:gray\">*The Sigmoid Function*</span></center></small>\n",
    "\n",
    "Let's plot this function so that we can see what we're dealing with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we'll import our mathematical toolkit `numpy` and the plotting tools `matplotlib` and `seaborn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define our sigmoid function.\n",
    "\n",
    "Note that we decorate it with `numpy`'s `vectorize` function so that the function can be called on lists and arrays without further modification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@np.vectorize\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally let us generate some points and create a plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 648x324 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.linspace(-10, 10, 1000)\n",
    "y = sigmoid(x)\n",
    "plt.plot(x,y, figure=plt.figure(figsize=(9,4.5)))\n",
    "plt.axvline(0, color='k', lw=0.5)\n",
    "plt.xlabel('$x$', size=15)\n",
    "plt.ylabel('$\\sigma(x)$', size=15);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us find the differential by applying the quotient rule. \n",
    "\n",
    "$$\\frac{d\\frac{u}{v}}{dx}=\\frac{v\\frac{du}{dx} - u\\frac{dv}{dx}}{v^2}$$\n",
    "\n",
    "<small><center><span style=\"color:gray\">*The Quotient Rule*</span></center></small>\n",
    "\n",
    "This leads us to the following derivative of the sigmoid function.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\sigma'(x) &=\\frac{e^{-x}}{(1+e^{-x})^2}\\\\&=\\frac{1}{1+e^{-x}}\\cdot \\frac{e^{-x}}{1+e^{-x}}\\\\&=\\frac{1}{1+e^{-x}}\\cdot\\frac{(1+e^{-x})-1}{1+e^{-x}}\\\\&=\\frac{1}{1+e^{-x}}\\cdot\\left(1-\\frac{1}{1+e^{-x}}\\right)\\\\&=\\sigma(x)(1-\\sigma(x))\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting A Problem To Solve\n",
    "\n",
    "To consider how neural networks work we need a set of data to learn from. In this case we will consider a a more complex classification problem wherein the outcome will be determined by some non-linear function of the inputs. This non-linear function is what we aim to learn with our network. First we must consider the relationship we wish to build. Let us consider a function of 4 variables. This will enable us to create a network as in the diagram at the top of this notebook (we will ultimately build a network with one hidden layer of 5 units).\n",
    "\n",
    "We let our input variables be random normal values ($a, b, c, d \\sim \\mathcal{N}(0,1)$) so that values are all on a standardised scale. This aids in the training of the network. While this scaling wont necessarily be the same in real data sets data is usually normalised (yielding so-called 'z' values) before training a neural network with it. Predictions that then be rescaled to return a meaningful value on the original scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.random.uniform(size=10000)\n",
    "b = np.random.uniform(size=10000)\n",
    "c = np.random.uniform(size=10000)\n",
    "d = np.random.uniform(size=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us define a function of these five variables that generates a surface separating positive and negative examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@np.vectorize\n",
    "def hyperplane(a,b,c,d):\n",
    "    value = sigmoid(0.1*a) + 0.4*sigmoid(b) - 0.2*sigmoid(c) + sigmoid(d)\n",
    "    return value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We apply this function to attain the outputs we will later seek to predict. We will refer to these as the true values as they are generated by the process we wish to learn. They are stored in the variable `true_values` accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_values = hyperplane(a,b,c,d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we put all of our data in one place. We generate a data frame with the inputs having their own columns and the outputs appended on the right. This is similar to how we may expect a real data set from an experiment to look and working with `pandas` should aid transition to any other work you may wish to pursue with neural nets (particularly if using `scikit-learn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.DataFrame(data={\n",
    "    'a':a,\n",
    "    'b':b,\n",
    "    'c':c,\n",
    "    'd':d,\n",
    "    'value': true_values\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quick peek at the data set to finalise the data initialisation and save us rereading this section when we just want to understand the structure of our data set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "      <th>c</th>\n",
       "      <th>d</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.915241</td>\n",
       "      <td>0.876958</td>\n",
       "      <td>0.505173</td>\n",
       "      <td>0.072895</td>\n",
       "      <td>1.198823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.651209</td>\n",
       "      <td>0.839387</td>\n",
       "      <td>0.396698</td>\n",
       "      <td>0.127875</td>\n",
       "      <td>1.207955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.560437</td>\n",
       "      <td>0.257284</td>\n",
       "      <td>0.839789</td>\n",
       "      <td>0.385774</td>\n",
       "      <td>1.195175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.680295</td>\n",
       "      <td>0.022301</td>\n",
       "      <td>0.435102</td>\n",
       "      <td>0.412747</td>\n",
       "      <td>1.199559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.282659</td>\n",
       "      <td>0.047266</td>\n",
       "      <td>0.276772</td>\n",
       "      <td>0.855818</td>\n",
       "      <td>1.299827</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          a         b         c         d     value\n",
       "0  0.915241  0.876958  0.505173  0.072895  1.198823\n",
       "1  0.651209  0.839387  0.396698  0.127875  1.207955\n",
       "2  0.560437  0.257284  0.839789  0.385774  1.195175\n",
       "3  0.680295  0.022301  0.435102  0.412747  1.199559\n",
       "4  0.282659  0.047266  0.276772  0.855818  1.299827"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"build\"></a>\n",
    "## Building the Neural Network\n",
    "### The Forward Pass\n",
    "\n",
    "Now that we have a problem we wish to apply our machine learner to we need to build the learner. In the case of the multi-layer perceptrons (relatively vanilla neural networks) we will be using here this is relatively simple.\n",
    "\n",
    "If one is familiar with summation notation and matrix algebra the mathematics and implementation of the forward pass is fairly straight forward. The formula for prediction is given below where $w_{a, b}$ denotes the weight from node $a$ to node $b$, $0$ denotes the bias weight in the respective layer, $Output$ denotes the output node, the $i$'s range over the inputs (of which there are $n_{input}$) and the $j$'s range over the nodes in the hidden layer (of which there are $n_{hidden})$.\n",
    "\n",
    "$$ prediction = w_{0,Output}+\\sum_{j=1}^{n_{hidden}} w_{j,Output}\\cdot \\sigma\\left(w_{0,j}+\\sum_{i=1}^{n_{input}} x_i w_{i,j}\\right)$$\n",
    "\n",
    "One memorable way of phrasing the way each layer in a neural networks works, (to my knowledge) coined by [Siraj Raval](https://www.youtube.com/channel/UCWN3xxRkmTPmbKwht9FuE5A), is as below.\n",
    "\n",
    "> Input times weights, plus a bias, activate!\n",
    "\n",
    "Put mathematically, take the matrix product of your vector of inputs and the matrix of weights for the layer in question (at each layer), add a bias and pass the result through your chosen activation function. The matrix form of the forward pass is wirtten below where $W_k$ denotes the weights matrix for layer $k$ and $b_k$ denotes the bias vector for each layer.\n",
    "\n",
    "$$ prediction = \\boldsymbol{b_2} + \\boldsymbol{W_2}\\left( \\sigma\\left(\\boldsymbol{b_1} + \\boldsymbol{W_1}\\boldsymbol{x}\\right)\\right)$$\n",
    "\n",
    "Note that for non-input layers the inputs are the outputs of the previous layer and that, in practical implementations, adding a bias is usually done by prepending a 1 to the start of your input vector and adding another row (of the biases) to the top of the weights matrix. This then deals with adding the bias through the matrix multiplication. Note that element $(i, j)$ of any given weight matrix (usually denoted $w_{i,j}$) is the weight from input $i$ on the sum in node $j$.\n",
    "\n",
    "Note that in the network built here the sigmoid function is used as an activation function. This limits the outputs of the given layer to be in the unit interval. However, we know from the data generation work above that the outputs will not necessarily be in this interval. Therefore in the final layer, in order not to limit the range of outputs, we will apply no activation at all, consider this like the case of the [plain perceptron](./01. The Perceptron.ipynb). This changes the formula above by taking out the sigmoid transformation ($\\sigma(\\cdot)$), this can also be considered by just changing the activation function in the final layer to an identity function ($f_{identity}(x) = x$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the formulae above we can write our prediction function. It takes the input vector $\\boldsymbol{x}$ and a list of weights matrices (one for each layer of the neural network) and conducts the arithmatic above.\n",
    "\n",
    "Initially a $1$ is inserted in the first position of the input vector $\\boldsymbol{x}$ to account for the bias (included here as an extra row of weights in the relevant weight matrices). Then for each intermediate (i.e. 'hidden') layer the matrix multiplication is conducted and the activation function applied to this output. To again account for an additive bias, a $1$ is prepended to the hidden layer output vector. Note that the final layer, having no activation function, is treated separately on the penultimate line of the function written below. The matrix multiplication is carried out as normal yet no activation is applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x, weights):\n",
    "    x = np.insert(x, 0, 1)\n",
    "    output = x\n",
    "    for layer in weights[:-1]:\n",
    "        output = sigmoid(np.matmul(output, layer))\n",
    "        output = np.insert(output, 0, 1)\n",
    "    output = np.matmul(output, weights[-1])\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us run a quick test example of how this preiction function works to make sure all is working well before we develop our training algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example\n",
    "For a working example we need the weights and some input. We will initialise the weights to be small random numbers. These are the weights we will start to learn from later on and hence we do not wish to include any big values as to do so imposes some strong prior view as to the relationship between the inputs and outputs.\n",
    "\n",
    "The weights for each layer will be stored in an $n\\times m$ matrix where $n$ is one greater than the number of nodes in the previous layer (one greater to account for the bias, in inputs can be considered a layer of nodes) and $m$ is the number of nodes in the subsequent layer (this time ignoring any bias 'node').\n",
    "\n",
    "Given that we are replicating the diagram at the top of this notebook which shows a network taking 4 inputs, having a hidden layer of 5 units and generates a single output our weight matrices will be of sizes $5\\times 5$ and $6\\times 1$ for the first and second layers respectively. Entry $i, j$ in each matrix can be considered to be the weight placed on input $i$ in the calculation at node $j$ for the respective layer. Note that when $i = 0$ (if using a 0 based indexing system) the weight can be considered a scalar bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = [\n",
    "        np.random.normal(0, 0.5, size=(5,5)),\n",
    "        np.random.normal(0, 0.5, size=(6,1))\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the relationship we are learning has been developed hear for pedagogic reasons it it not necessarily meaningful with the focus being on the mathematics of modelling. Therefore we can just choose some fairly random number from the ranges included in the inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = np.array([0.003, 0.19, 0.1, 0.39])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we can run our prediction with our chosen inputs and the random weights. Note that the output is a 1-vector (a numpy array) rather than a raw scalar value since it is the result of matrix multiplication. This means that the network can be generalised to have multiple outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.3672341])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(i, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Should you wish to consolidate your understanding of how this network works the list of weight matrices is given below so that you may trace/check the calculation yourself outside of Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-0.35488876, -0.23659869, -0.33867314,  0.36387468,  0.32611655],\n",
       "        [-0.26065506, -1.05467748,  0.18559385,  0.25842844, -0.40552946],\n",
       "        [-0.08049153, -0.62635345,  0.52101412, -0.08069795, -0.83872539],\n",
       "        [-0.72111432, -0.29579596, -0.27537176,  0.66396855,  0.20578943],\n",
       "        [ 0.52081198, -0.68363231,  0.01350068, -0.11467529,  0.26864561]]),\n",
       " array([[ 0.03878111],\n",
       "        [ 0.70394749],\n",
       "        [-0.84874312],\n",
       "        [ 0.50273702],\n",
       "        [-0.8350314 ],\n",
       "        [-0.26286314]])]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"backpropagation\"></a>\n",
    "## Backpropagation\n",
    "Now comes the more complicated part. Training the model.\n",
    "\n",
    "The [forward pass](#build) presented above is also known as forward propagation as the calculation at each stage is propagated forwards into the next  calculation. For the model to learn it must learn from the errors made. These errors are the differences between the predictions and the known values. Therefore the errors are clearly measured at the output of the network. The errors being noted at the end of the calculation but the parameters we wish to vary to reduce the errors (namely the weights in each layer) occuring earlier in the calculation requires that the errors and information from later layers be passed back to earlier layers in order for learning to occur. This led to the backpropagation algorithm whereby the errors and information from any later layers are used when training a given layer.\n",
    "\n",
    "Before we lay out the mathematics of the training algorithm let us first consider the intuition of the process we are about to undertake. We will define a loss function that is proportional to the prediction error (but may contain other penalties or weight certain errors more heavily than others). This loss function will be defined over the possible values for the weights in our model. The aim is therefore to minimise the loss function (thereby reducing prediction errors) by varying the weights within our neural network.\n",
    "\n",
    "The approach we will take is gradient descent. Let me provide the intuition as follows. Imagine that there are only two weights over which you wish to minimise the loss. The loss is therefore defined at all possible pairings of weight values. Given that the weights are continuous variables the space of weights and the related losses can be plotted in three-dimensional space with the 'floor' being coordinates in $w_1, w_2$ space and the hight at any point being given by $loss = L(w_1, w_2)$. We csn imagine this to be like a mountainous landscape. The weights are initialised at a random point $(w_1^{start}, w_2^{start})$ from which we wish to get to the optimum $(w_1^*,w_2^*)$ which will be the lowest point on the surface. Mathematically we are looking to find $\\min_{(w_1,w_2)}{L(w_1, w_2)}$. We will get there step by step by moving 'downhill'. This is done by calculating the gradient at our current position and finding the steepest way down and then taking a step of given size in that direction before then reevaluating the gradient and then continuing on. This descent will bring us down until we reach a minimum or we run out of steps/time.\n",
    "\n",
    "Note that in the above we end at _a minimum_ not the minimum. The problem of arriving at a local minimum is one that is greatly explored in all optimisation including in machine learning. This alongside problems around overfitting, choosing the step size and many more elements of designing and implementing an optimisation algorithm will be left for a later day. Today we will focus on the mathematics of our chosen (standard) optimisation algorithm. The key to this is finding the gradient we wish to take a step in the direction of. This gradient (of the loss function) is given by the vector below.\n",
    "\n",
    "$$\\nabla L(\\boldsymbol{w}) = \\left[\\frac{\\partial L}{\\partial w_{0,1}}, \\frac{\\partial L}{\\partial w_{0,2}}, \\cdots, \\frac{\\partial L}{\\partial w_{n_{hidden},Output}}\\right]$$\n",
    "\n",
    "Now we may define this loss function and find the derivatives. Once we have this information we will be able to train our multi-layer perceptron."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use a simple standard loss function of minmising the sum of squared residuals. We will scale this loss to make some of the later mathematics easier. We define the loss across all datapoints $d$ in the available data set $D$ as below.\n",
    "\n",
    "$$Loss = \\frac{1}{2} \\sum_{d\\in D} \\left(\\hat{y}_d - y^{\\ true}_d\\right)^2$$\n",
    "\n",
    "Where the model prediction is given by $\\hat{y}$ and the target value is given by $y^{\\ true}$.\n",
    "\n",
    "Now that we have the loss function along with a [formula](#build) for the prediction as a function of the weights we may take derivatives to build up the gradient vector.\n",
    "\n",
    "Firstly let us remind ourselves of the forward pass formula.\n",
    "\n",
    "$$ prediction = w_{0,Output}+\\sum_{j=1}^{n_{hidden}} w_{j,Output}\\cdot \\sigma\\left(w_{0,j}+\\sum_{i=1}^{n_{input}} x_i w_{i,j}\\right)$$\n",
    "\n",
    "At a high level the derivative of the loss function with respect to some weight $w_{i,j}$ will be given by the following.\n",
    "\n",
    "$$\\frac{\\partial Loss}{\\partial w_{i,j}} = \\sum_{d\\in D} \\left(\\frac{\\partial \\hat{y}_d}{\\partial w_{i,j}} \\left(\\hat{y}_d - y^{\\ true}_d\\right)\\right)$$\n",
    "\n",
    "Note that the $\\frac{1}{2}$ we scaled the loss by cancels with the power of $2$ now that we take the first derivative, this saves us from juggling constants without losing anything of consequence.\n",
    "\n",
    "The non-trivial part of this derivative is the term in the derivative of $\\hat{y}$. Let's focus on this term and rebuild the full derivative once we have all the parts.\n",
    "\n",
    "First considering the simpler case of an element of the final layer (i.e. this derivative for weights between the hidden layer and the output).\n",
    "\n",
    "$$\\frac{\\partial\\ \\hat{y}}{\\partial w_{j, Output}} = net_j$$\n",
    "\n",
    "Where $net_j$ denotes the value of the $j^{th}$ unit in the hidden layer and $w_{j, Ouput}$ denotes the weight between the $j^{th}$ node of the hidden layer and the output node. To clarify:\n",
    "\n",
    "$$net_j = \\sigma\\left(w_{0,j}+\\sum_{i=1}^n x_i w_{i,j}\\right)$$\n",
    "\n",
    "The more complex derivatives are those concerning the weights earlier in the tree. This makes sense as they do not affect the prediction so directly as they influence the values in the hidden layer which then affect the output through another layer of calculation. Therefore these weights have a more in direct impact and this generates a more complex derivative for the prediction depending on more terms. This derivative is given below.\n",
    "\n",
    "$$\\frac{\\partial\\ \\hat{y}}{\\partial w_{i,j}}=\\sum_{j=0}^{n_{hidden}} \\left(w_{j,Output}\\cdot \\frac{\\partial net_j}{\\partial w_{i,j}}\\right)$$\n",
    "\n",
    "Where (applying the fact that $\\sigma' (x) = \\sigma (x) (1 - \\sigma(x))$):\n",
    "\n",
    "$$\\frac{\\partial net_j}{\\partial w_{i,j}} = x_i \\sigma\\left(w_{0,j}+\\sum_{i=1}^{n_{input}} x_i w_{i,j}\\right) \\left(1 - \\sigma\\left(w_{0,j}+\\sum_{i=1}^{n_{input}} x_i w_{i,j}\\right)\\right)$$\n",
    "\n",
    "Combining the above two formulae we get the overall formula for the gradient of the prediction with respect to the weights in the first layer of our network.\n",
    "\n",
    "$$\\frac{\\partial\\ \\hat{y}}{\\partial w_{i,j}}=\\sum_{j=1}^n w_{j,Output} x_{i} \\cdot \\sigma\\left(w_{0,j}+\\sum_{i=1}^n x_i w_{i,j}\\right)\\left(1 - \\sigma\\left(w_{0,j}+\\sum_{i=1}^n x_i w_{i,j}\\right)\\right)$$\n",
    "\n",
    "Now that the have the formulae for the derivatives we require let us add the following notation to have repetition of these lengthy formulae.\n",
    "\n",
    "$$\\nabla \\hat{y} = \\left[\\frac{\\partial\\ \\hat{y}}{\\partial w_{0,1}}, \\frac{\\partial\\ \\hat{y}}{\\partial w_{0,2}}, \\cdots, \\frac{\\partial\\ \\hat{y}}{\\partial w_{n_{hidden},Output}}\\right]$$\n",
    "\n",
    "We may now therefore see that\n",
    "$$\\nabla L(\\boldsymbol{w}) = \\sum_{d \\in D} \\nabla \\hat{y}_d\\cdot (\\hat{y}_d - y_d^{\\ true})$$\n",
    "\n",
    "We shall however apply the average rather than the sum of the error terms (which is simply a change in scale).\n",
    "\n",
    "Furthermore we wish to apply gradient *descent* therefore we will update the weights as below. Note the negative sign as we wish to move down the loss function to a minimum rather than follow the gradient upwards.\n",
    "\n",
    "$$ \\boldsymbol{w} \\leftarrow \\boldsymbol{w} - \\eta\\ \\nabla L(\\boldsymbol{w})$$\n",
    "\n",
    "The update at each step is therefore a step of size $\\eta$ in the direction of the gradient of the loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To attain these results practically we will need to attain the values of nodes at intermediate stages of our netowkr (i.e. the $net_j$ values). Let us write a function that only partially completes the computation of our netural network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_intermediate_outputs(x, weights, layer):\n",
    "    x = np.insert(x, 0, 1)\n",
    "    output = x\n",
    "    full_network = False\n",
    "    if layer == len(weights):\n",
    "        layer -= 1\n",
    "        full_network = True\n",
    "    for i in range(layer):\n",
    "        output = sigmoid(np.matmul(output, weights[i]))\n",
    "        output = np.insert(output, 0, 1)\n",
    "    if full_network:\n",
    "        output = np.matmul(output, weights[-1])\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can compute the gradients and place them in a list that aligns with the weights to which the gradients apply. \n",
    "\n",
    "See comments within the function for more information.\n",
    "\n",
    "Note that this function could be made more efficient by vectorising the for loops but it works and is potentially clearer for didactic purposes and hence is left as is in the initial implementation for the moment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_gradients(inputs, weights):\n",
    "    # Finding the number of layers in our network\n",
    "    no_layers = len(weights)\n",
    "    \n",
    "    # Attaining predictions to ultimately attain errors.\n",
    "    predictions = predict(inputs, weights)\n",
    "    \n",
    "    # Since we know the final layer derivatives are simply equal to the values of the nodes in the hidden\n",
    "    # layer we can get these values directly using the function written previously.\n",
    "    final_layer_derivatives = get_intermediate_outputs(inputs, weights, no_layers - 1).reshape(weights[-1].shape)\n",
    "    \n",
    "    # Attaining the values output from the first layer to help in constructing the gradients from here.\n",
    "    # These are the net values (again)\n",
    "    first_layer_outputs = get_intermediate_outputs(inputs, weights, 1)\n",
    "    \n",
    "    # Initialising an array to hold the gradients\n",
    "    previous_layer_derivatives = np.array([])\n",
    "    \n",
    "    # Looping over hidden layer nodes (j) and inputs(i)\n",
    "    for j in range(weights[-2].shape[1]):\n",
    "        for i in range(weights[-2].shape[0]):\n",
    "            # Applying the formula for the gradient as given in the text above\n",
    "            \n",
    "            # If we are handling a bias the gradient is the same calculation but forcing the input to be 0\n",
    "            if i == 0:\n",
    "                gradient = 1 * weights[-1][j] * first_layer_outputs[j+1] * (1 - first_layer_outputs[j+1])\n",
    "            else:\n",
    "                gradient = inputs[i-1] * weights[-1][j] * first_layer_outputs[j+1] * (1 - first_layer_outputs[j+1])\n",
    "                \n",
    "            # Adding our newfound gradient to the array\n",
    "            previous_layer_derivatives = np.append(previous_layer_derivatives, gradient)\n",
    "            \n",
    "    # Reshaping our gradients array to align with the weights matrix to which it applies\n",
    "    previous_layer_derivatives = np.reshape(previous_layer_derivatives, weights[-2].shape)\n",
    "    \n",
    "    # Returning the gradients in a two arrays, one for each layer\n",
    "    return previous_layer_derivatives, final_layer_derivatives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can check the output from the above function to see if it makes sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 9.55704063e-03,  2.86711219e-05,  1.81583772e-03,\n",
       "          9.55704063e-04,  3.72724585e-03],\n",
       "        [ 1.58384890e-01,  4.75154670e-04,  3.00931291e-02,\n",
       "          1.58384890e-02,  6.17701071e-02],\n",
       "        [-2.08602121e-01, -6.25806364e-04, -3.96344031e-02,\n",
       "         -2.08602121e-02, -8.13548273e-02],\n",
       "        [ 1.21456945e-01,  3.64370835e-04,  2.30768195e-02,\n",
       "          1.21456945e-02,  4.73682085e-02],\n",
       "        [-2.04403191e-01, -6.13209574e-04, -3.88366063e-02,\n",
       "         -2.04403191e-02, -7.97172446e-02]]), array([[1.        ],\n",
       "        [0.44029649],\n",
       "        [0.34187132],\n",
       "        [0.43502067],\n",
       "        [0.59169838],\n",
       "        [0.57221471]]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_gradients(i, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are finally ready to put all of our work together to implement the full back propagation algorithm. We loop over the dataset to get our predictions and then calculate our errors. The gradients are then attained and a step of size eta ($\\eta$) is taken in that direction.\n",
    "\n",
    "Each pass over the data set is known as an epoch, the step size is also known as the learning rate and is often referred to by the Greek letter eta ($\\eta$).\n",
    "\n",
    "We print out the errors so that we can see the progress of our model. Feel free to play with the step size and number of epochs in the implementation below to see if the results follow your intutition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backprop(inputs, weights, true_values, epochs, eta):\n",
    "    w = np.copy(weights)\n",
    "    for _ in range(epochs):\n",
    "        predictions = np.array([])\n",
    "        errors = np.array([])\n",
    "        gradients_l1 = np.array([w[0]])\n",
    "        gradients_l2 = np.array([w[1]])\n",
    "        for i, t in zip(inputs, true_values):\n",
    "            prediction = predict(i, w)\n",
    "            predictions = np.append(predictions, prediction)\n",
    "            errors = np.append(errors, prediction - t)\n",
    "            g1, g2 = calculate_gradients(i, w)\n",
    "            gradients_l1 = np.vstack((gradients_l1, [g1]))\n",
    "            gradients_l2 = np.vstack((gradients_l2, [g2]))\n",
    "        \n",
    "        grads = [errors.mean() * gradients_l1.mean(axis=0), errors.mean() * gradients_l2.mean(axis=0)]\n",
    "        predictions = np.array(predictions)\n",
    "        rmse = np.sqrt(((predictions - true_values)**2).mean())\n",
    "        print(rmse)\n",
    "\n",
    "        for l in range(len(w)):\n",
    "            w[l] -= eta * grads[l]\n",
    "    return w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we collect our data in numpy arrays and apply backprop using 100 epochs and an initial learning rate of 0.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = dataset[['a','b','c','d']].values\n",
    "y = dataset['value'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.578374146804774\n",
      "0.9312349021539885\n",
      "0.5504270612951493\n",
      "0.3250227156553526\n",
      "0.19379625178844223\n",
      "0.11996587444471339\n",
      "0.08115042336692907\n",
      "0.0629904988895241\n",
      "0.05559431277355389\n",
      "0.05287504823997944\n",
      "0.05191990388661968\n",
      "0.05158625746082776\n",
      "0.05146744701966655\n",
      "0.05142347747905161\n",
      "0.05140626381138705\n",
      "0.05139903472974495\n",
      "0.0513957602600725\n",
      "0.05139416917360941\n",
      "0.05139335085545238\n",
      "0.05139291233823147\n",
      "0.05139267084067152\n",
      "0.051392535541941925\n",
      "0.051392458948078694\n",
      "0.051392415319202835\n",
      "0.05139239037771956\n"
     ]
    }
   ],
   "source": [
    "weights_learned = backprop(x, w, y, 25, 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the root mean squared error (RMSE) is getting smaller and looks as though progress is leveling off at a RMSE of around 0.05. Extend the number of epochs and rerun the code to see how good a model you can produce. \n",
    "\n",
    "Note that the relationship we are trying to learn here is without noise so overfitting concerns are small and a very good model should be possible to attain.\n",
    "\n",
    "We will try to improve our model by continuing training with a lower learning rate meaning more fine tuning of the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05139237608939999\n",
      "0.051392371990503886\n",
      "0.05139236876535353\n",
      "0.05139236622740657\n",
      "0.05139236423005673\n",
      "0.0513923626580425\n",
      "0.051392361420719686\n",
      "0.05139236044678768\n",
      "0.05139235968015157\n",
      "0.05139235907667318\n",
      "0.05139235860161863\n",
      "0.0513923582276522\n",
      "0.05139235793325921\n",
      "0.0513923577015054\n",
      "0.05139235751906127\n"
     ]
    }
   ],
   "source": [
    "weights_learned = backprop(x, weights_learned, y, 15, 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further training has produced little progress sugesting that we are around the optimal solution our chosen model architecture can create.\n",
    "\n",
    "Let us finalise our analysis by appending a column of predictions to out `dataset` dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = []\n",
    "for i in x: y_hat.append(predict(i, weights_learned)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "      <th>c</th>\n",
       "      <th>d</th>\n",
       "      <th>value</th>\n",
       "      <th>y hat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.915241</td>\n",
       "      <td>0.876958</td>\n",
       "      <td>0.505173</td>\n",
       "      <td>0.072895</td>\n",
       "      <td>1.198823</td>\n",
       "      <td>1.259136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.651209</td>\n",
       "      <td>0.839387</td>\n",
       "      <td>0.396698</td>\n",
       "      <td>0.127875</td>\n",
       "      <td>1.207955</td>\n",
       "      <td>1.265691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.560437</td>\n",
       "      <td>0.257284</td>\n",
       "      <td>0.839789</td>\n",
       "      <td>0.385774</td>\n",
       "      <td>1.195175</td>\n",
       "      <td>1.154314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.680295</td>\n",
       "      <td>0.022301</td>\n",
       "      <td>0.435102</td>\n",
       "      <td>0.412747</td>\n",
       "      <td>1.199559</td>\n",
       "      <td>1.222694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.282659</td>\n",
       "      <td>0.047266</td>\n",
       "      <td>0.276772</td>\n",
       "      <td>0.855818</td>\n",
       "      <td>1.299827</td>\n",
       "      <td>1.303933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.300819</td>\n",
       "      <td>0.980814</td>\n",
       "      <td>0.318806</td>\n",
       "      <td>0.996610</td>\n",
       "      <td>1.413013</td>\n",
       "      <td>1.411764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.063727</td>\n",
       "      <td>0.616757</td>\n",
       "      <td>0.848085</td>\n",
       "      <td>0.298776</td>\n",
       "      <td>1.195496</td>\n",
       "      <td>1.127035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.943742</td>\n",
       "      <td>0.417800</td>\n",
       "      <td>0.793604</td>\n",
       "      <td>0.591618</td>\n",
       "      <td>1.270774</td>\n",
       "      <td>1.238789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.434185</td>\n",
       "      <td>0.968384</td>\n",
       "      <td>0.128421</td>\n",
       "      <td>0.601513</td>\n",
       "      <td>1.340362</td>\n",
       "      <td>1.396442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.737530</td>\n",
       "      <td>0.995429</td>\n",
       "      <td>0.733167</td>\n",
       "      <td>0.031645</td>\n",
       "      <td>1.183304</td>\n",
       "      <td>1.210274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.143288</td>\n",
       "      <td>0.773841</td>\n",
       "      <td>0.173265</td>\n",
       "      <td>0.371768</td>\n",
       "      <td>1.260567</td>\n",
       "      <td>1.300927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.585480</td>\n",
       "      <td>0.993777</td>\n",
       "      <td>0.818602</td>\n",
       "      <td>0.847902</td>\n",
       "      <td>1.367905</td>\n",
       "      <td>1.296828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.424618</td>\n",
       "      <td>0.408354</td>\n",
       "      <td>0.947508</td>\n",
       "      <td>0.243112</td>\n",
       "      <td>1.167249</td>\n",
       "      <td>1.114008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.646919</td>\n",
       "      <td>0.359743</td>\n",
       "      <td>0.448914</td>\n",
       "      <td>0.881986</td>\n",
       "      <td>1.336916</td>\n",
       "      <td>1.331845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.232987</td>\n",
       "      <td>0.458348</td>\n",
       "      <td>0.555285</td>\n",
       "      <td>0.866850</td>\n",
       "      <td>1.327891</td>\n",
       "      <td>1.286791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.639018</td>\n",
       "      <td>0.435043</td>\n",
       "      <td>0.526735</td>\n",
       "      <td>0.503069</td>\n",
       "      <td>1.256237</td>\n",
       "      <td>1.259781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.136409</td>\n",
       "      <td>0.200168</td>\n",
       "      <td>0.459560</td>\n",
       "      <td>0.757791</td>\n",
       "      <td>1.281652</td>\n",
       "      <td>1.248908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.368640</td>\n",
       "      <td>0.684299</td>\n",
       "      <td>0.157880</td>\n",
       "      <td>0.822392</td>\n",
       "      <td>1.361960</td>\n",
       "      <td>1.396713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.441274</td>\n",
       "      <td>0.818580</td>\n",
       "      <td>0.738241</td>\n",
       "      <td>0.510918</td>\n",
       "      <td>1.278303</td>\n",
       "      <td>1.241548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.529520</td>\n",
       "      <td>0.513486</td>\n",
       "      <td>0.960989</td>\n",
       "      <td>0.318192</td>\n",
       "      <td>1.197704</td>\n",
       "      <td>1.145964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.334961</td>\n",
       "      <td>0.313747</td>\n",
       "      <td>0.314233</td>\n",
       "      <td>0.605725</td>\n",
       "      <td>1.270874</td>\n",
       "      <td>1.282683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.396561</td>\n",
       "      <td>0.243641</td>\n",
       "      <td>0.011509</td>\n",
       "      <td>0.020862</td>\n",
       "      <td>1.138797</td>\n",
       "      <td>1.223808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.174515</td>\n",
       "      <td>0.922508</td>\n",
       "      <td>0.540561</td>\n",
       "      <td>0.691979</td>\n",
       "      <td>1.330602</td>\n",
       "      <td>1.301395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.004347</td>\n",
       "      <td>0.780349</td>\n",
       "      <td>0.701450</td>\n",
       "      <td>0.376250</td>\n",
       "      <td>1.233677</td>\n",
       "      <td>1.182341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.831463</td>\n",
       "      <td>0.591009</td>\n",
       "      <td>0.243796</td>\n",
       "      <td>0.982047</td>\n",
       "      <td>1.393598</td>\n",
       "      <td>1.419596</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           a         b         c         d     value     y hat\n",
       "0   0.915241  0.876958  0.505173  0.072895  1.198823  1.259136\n",
       "1   0.651209  0.839387  0.396698  0.127875  1.207955  1.265691\n",
       "2   0.560437  0.257284  0.839789  0.385774  1.195175  1.154314\n",
       "3   0.680295  0.022301  0.435102  0.412747  1.199559  1.222694\n",
       "4   0.282659  0.047266  0.276772  0.855818  1.299827  1.303933\n",
       "5   0.300819  0.980814  0.318806  0.996610  1.413013  1.411764\n",
       "6   0.063727  0.616757  0.848085  0.298776  1.195496  1.127035\n",
       "7   0.943742  0.417800  0.793604  0.591618  1.270774  1.238789\n",
       "8   0.434185  0.968384  0.128421  0.601513  1.340362  1.396442\n",
       "9   0.737530  0.995429  0.733167  0.031645  1.183304  1.210274\n",
       "10  0.143288  0.773841  0.173265  0.371768  1.260567  1.300927\n",
       "11  0.585480  0.993777  0.818602  0.847902  1.367905  1.296828\n",
       "12  0.424618  0.408354  0.947508  0.243112  1.167249  1.114008\n",
       "13  0.646919  0.359743  0.448914  0.881986  1.336916  1.331845\n",
       "14  0.232987  0.458348  0.555285  0.866850  1.327891  1.286791\n",
       "15  0.639018  0.435043  0.526735  0.503069  1.256237  1.259781\n",
       "16  0.136409  0.200168  0.459560  0.757791  1.281652  1.248908\n",
       "17  0.368640  0.684299  0.157880  0.822392  1.361960  1.396713\n",
       "18  0.441274  0.818580  0.738241  0.510918  1.278303  1.241548\n",
       "19  0.529520  0.513486  0.960989  0.318192  1.197704  1.145964\n",
       "20  0.334961  0.313747  0.314233  0.605725  1.270874  1.282683\n",
       "21  0.396561  0.243641  0.011509  0.020862  1.138797  1.223808\n",
       "22  0.174515  0.922508  0.540561  0.691979  1.330602  1.301395\n",
       "23  0.004347  0.780349  0.701450  0.376250  1.233677  1.182341\n",
       "24  0.831463  0.591009  0.243796  0.982047  1.393598  1.419596"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['y hat'] = y_hat\n",
    "dataset.head(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may see that occasionally our model works well. However in general it is not a great predictive model. The RMSE may seem small but in terms of the scale of the data it is probably an average error of 5-10%. Not great performance when modelling a known determanistic process.\n",
    "\n",
    "How could we improve the model?\n",
    "We could update the model architecture, try alternative activation functions, a greater number of nodes in the hidden layer and more layers within the model all of which will change and potentially augement the expressive power of our model.\n",
    "\n",
    "A more likely avenue for improvement is to improve the model through incremental training or stochastic gradient descent. The incremental case involves updating the weights after each data point had been passed through the back propagation algorithm rather than aggregating and updating with averages. Stochastic gradient descent involves updating after taking a sample of the data points and calculating the gradients for the sample only before averaging and updating. This allows some sampling noise into training which can help to avoid training towards the average case alone.\n",
    "\n",
    "In future this notebook may be updated to handle such cases (in fact what we have done here for training is less frequently seen than the two cases noted above as improvements).\n",
    "\n",
    "However, the notebook aims to be a learning tool for those who wish to understand how neural networks work and how they can be trained. Our algorithm works with limited success and taking a slightly alternative approch means that one may now truly consider how back propagation works rather than simply seeing the standard notes and approach and taking it for granted.\n",
    "\n",
    "All feedback, comments and advice welcome."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
