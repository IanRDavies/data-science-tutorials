{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Multi-Layer Perceptron\n",
    "Any neural network is simply a series of perceptrons feeding into one another. The perceptrons are arranged in layers with the outputs of a previous layer being the inputs into the next layer.\n",
    "\n",
    "With this in mind it seems that we did much of the heavy lifting in the [first part](./01. The Perceptron.ipynb) concerning the percepton - the single neuron emultating unit that is the building block of a neural network.\n",
    "\n",
    "In this workbook we will consider the simplest case of a fully-connected network. That is a network where each node (percepton) in a given layer is connected to every node in the subsequent layer. This means that the output of a perceptron in the first layer will therefore be an input into each perceptron in the next layer.\n",
    "\n",
    "![Neural Network Diagram](./img/neural-network.png \"A Fully Connected Neural Network\")\n",
    "\n",
    "<center><span style=\"color:gray\">*Source: TEXample.net*</span></center>\n",
    "\n",
    "Similarly to the case of the vanilla perceptron we considered before the values passed as inputs to each layer will have weights associated with them. These weights will be the variables which we will vary as part of the training process with the ultimate goal of loss minimisation.\n",
    "\n",
    "## Training a Neural Network vs Training a Perceptron\n",
    "In contast to a single perceptron, a multi-layer perceptron must be trained in a more complex way since the weights in the first layer only have an indirect impact on the output. This is then itself difficult should the step activation function be kept. This discontinuous function is not differentiable and hence we are unable to perform what's known as backpropagation. Backpropagation is the algorithmic way of training a neural network. Errors in the output are traced back through the layers of the network and the weights are updated in turn in order to reduce the error. We will consider this process in detail later. \n",
    "\n",
    "In order to undertake backpropagation the error, or more specifically the model prediction, needs to be differentiable with respect to the weights. In order to achieve this a smooth (differentiable) activation function is used in place of the perceptron step function.\n",
    "\n",
    "These functions are known as activation functions because they play the role of the activation threshold in the perceptron or the neuron being emulated. Popular choices are the sigmoid function, tanh and the Rectified Linear Unit function (ReLU). "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
